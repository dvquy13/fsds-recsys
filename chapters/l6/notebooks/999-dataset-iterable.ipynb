{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "583aefbe-a998-46c0-8ce1-a176b6e12158",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30ba6759-81b5-41fa-ac29-4862645964da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from src.skipgram.dataset import SkipGramDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cad590b-1033-46f6-bc2c-6eb55652e572",
   "metadata": {},
   "source": [
    "# Mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "887d856a-cea3-4968-adc0-8c7b2c473819",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\n",
    "    [\"b\", \"c\", \"d\", \"e\", \"a\"],\n",
    "    [\"f\", \"b\", \"b\", \"b\", \"k\"],\n",
    "    [\"g\", \"m\", \"k\", \"l\", \"h\"],\n",
    "    [\"b\", \"c\", \"k\"],\n",
    "    [\"j\", \"i\", \"c\"],\n",
    "]\n",
    "\n",
    "with open(\"sequences.jsonl\", \"w\") as f:\n",
    "    for sequence in sequences:\n",
    "        f.write(json.dumps(sequence) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6799dd3-3b14-4d82-98bb-a35d02fab601",
   "metadata": {},
   "source": [
    "# Test Iterable Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6afff45-a30e-4ad8-a005-354ed429bc1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8089b131564dde97e18397efa0bd83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lines: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n",
      "c\n",
      "d\n",
      "e\n",
      "a\n",
      "f\n",
      "b\n",
      "b\n",
      "b\n",
      "k\n",
      "g\n",
      "m\n",
      "k\n",
      "l\n",
      "h\n",
      "b\n",
      "c\n",
      "k\n",
      "j\n",
      "i\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "class LargeSequenceDataset(IterableDataset):\n",
    "    def __init__(self, file_path):\n",
    "        super(LargeSequenceDataset, self).__init__()\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Open the file and read line by line\n",
    "        with open(self.file_path, \"r\") as f:\n",
    "            # Wrap the file with tqdm to show progress\n",
    "            for line in tqdm(f, desc=\"Processing lines\"):\n",
    "                # Parse each line into a Python object\n",
    "                sequence = json.loads(line)\n",
    "                for item in sequence:\n",
    "                    yield item\n",
    "\n",
    "\n",
    "# Usage\n",
    "train_dataset = LargeSequenceDataset(\"sequences.jsonl\")\n",
    "\n",
    "# Example of loading data with a progress bar\n",
    "for data in train_dataset:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27211cd9-2ede-4464-8443-c5381b0fc679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835616606b7841dba5cb0d6ad2c12648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lines: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'c']\n",
      "['d', 'e']\n",
      "['a', 'f']\n",
      "['b', 'b']\n",
      "['b', 'k']\n",
      "['g', 'm']\n",
      "['k', 'l']\n",
      "['h', 'b']\n",
      "['c', 'k']\n",
      "['j', 'i']\n",
      "['c']\n"
     ]
    }
   ],
   "source": [
    "# DataLoader for batching\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2)\n",
    "\n",
    "# Example of iterating through DataLoader\n",
    "for batch in train_loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c7363a-0bbd-47ac-aaee-414389c01acc",
   "metadata": {},
   "source": [
    "# Test SkipGram implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "055407bb-c039-4f0c-863e-8785acd16281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-29 15:44:10.102\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.skipgram.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mProcessing sequences to build interaction data...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7fe23127b14a7996e9874e098acb83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building interactions: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simulate pre-configured id_to_idx mapper\n",
    "id_to_idx = {\n",
    "    id_: idx for id_, idx in zip(list(string.ascii_letters[:13]), list(range(13)))\n",
    "}\n",
    "id_to_idx[\"a\"] = 1\n",
    "id_to_idx[\"b\"] = 0\n",
    "\n",
    "# Create dataset with frequency-based negative sampling\n",
    "dataset = SkipGramDataset(\n",
    "    \"sequences.jsonl\", window_size=1, negative_samples=2, id_to_idx=id_to_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea4648e5-6ad2-4c7b-85f7-4ab054d67447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target_items': tensor([0, 0, 0, 2, 2, 2, 2, 2, 2]), 'context_items': tensor([ 2,  6,  7,  0,  3,  6,  5, 11,  5]), 'labels': tensor([1., 0., 0., 1., 1., 0., 0., 0., 0.])}\n",
      "{'target_items': tensor([3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4]), 'context_items': tensor([ 2,  4,  9, 10,  5, 10,  3,  1,  6,  8,  6,  5]), 'labels': tensor([1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.])}\n",
      "{'target_items': tensor([1, 1, 1, 5, 5, 5]), 'context_items': tensor([ 4, 12,  9,  0,  1,  2]), 'labels': tensor([1., 0., 0., 1., 0., 0.])}\n",
      "{'target_items': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'context_items': tensor([ 5,  0,  9, 11,  9,  6,  0,  0,  9,  7,  6,  9]), 'labels': tensor([1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.])}\n",
      "{'target_items': tensor([ 0,  0,  0,  0,  0,  0, 10, 10, 10]), 'context_items': tensor([ 0, 10,  8,  6,  8, 12,  0,  1,  8]), 'labels': tensor([1., 1., 0., 0., 0., 0., 1., 0., 0.])}\n",
      "{'target_items': tensor([ 6,  6,  6, 12, 12, 12, 12, 12, 12]), 'context_items': tensor([12,  0,  5,  6, 10,  4,  0,  3,  9]), 'labels': tensor([1., 0., 0., 1., 1., 0., 0., 0., 0.])}\n",
      "{'target_items': tensor([10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11]), 'context_items': tensor([12, 11,  9,  8,  4,  3, 10,  7,  2,  3,  2,  4]), 'labels': tensor([1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.])}\n",
      "{'target_items': tensor([7, 7, 7, 0, 0, 0]), 'context_items': tensor([11,  5,  0,  2,  6, 11]), 'labels': tensor([1., 0., 0., 1., 0., 0.])}\n",
      "{'target_items': tensor([ 2,  2,  2,  2,  2,  2, 10, 10, 10]), 'context_items': tensor([ 0, 10,  6, 12,  6, 12,  2,  8,  9]), 'labels': tensor([1., 1., 0., 0., 0., 0., 1., 0., 0.])}\n",
      "{'target_items': tensor([9, 9, 9, 8, 8, 8, 8, 8, 8]), 'context_items': tensor([ 8, 10,  0,  9,  2, 10, 12, 10,  4]), 'labels': tensor([1., 0., 0., 1., 1., 0., 0., 0., 0.])}\n",
      "{'target_items': tensor([2, 2, 2]), 'context_items': tensor([8, 5, 6]), 'labels': tensor([1., 0., 0.])}\n",
      "___\n",
      "{'target_items': tensor([0, 0, 0, 2, 2, 2, 2, 2, 2]), 'context_items': tensor([ 2,  7,  8,  0,  3,  5, 12, 12, 11]), 'labels': tensor([1., 0., 0., 1., 1., 0., 0., 0., 0.])}\n",
      "{'target_items': tensor([3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4]), 'context_items': tensor([ 2,  4, 10, 11,  5, 12,  3,  1,  8, 10,  5, 11]), 'labels': tensor([1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.])}\n",
      "{'target_items': tensor([1, 1, 1, 5, 5, 5]), 'context_items': tensor([ 4,  7, 11,  0,  1,  9]), 'labels': tensor([1., 0., 0., 1., 0., 0.])}\n",
      "{'target_items': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'context_items': tensor([ 5,  0,  6,  8,  6,  8,  0,  0, 12,  7,  9,  7]), 'labels': tensor([1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.])}\n",
      "{'target_items': tensor([ 0,  0,  0,  0,  0,  0, 10, 10, 10]), 'context_items': tensor([ 0, 10, 12, 11, 11,  8,  0,  4,  9]), 'labels': tensor([1., 1., 0., 0., 0., 0., 1., 0., 0.])}\n",
      "{'target_items': tensor([ 6,  6,  6, 12, 12, 12, 12, 12, 12]), 'context_items': tensor([12,  3,  2,  6, 10,  0,  2,  0,  2]), 'labels': tensor([1., 0., 0., 1., 1., 0., 0., 0., 0.])}\n",
      "{'target_items': tensor([10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11]), 'context_items': tensor([12, 11,  1,  9,  8,  4, 10,  7,  4,  2,  3,  0]), 'labels': tensor([1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.])}\n",
      "{'target_items': tensor([7, 7, 7, 0, 0, 0]), 'context_items': tensor([11,  3,  1,  2,  6,  9]), 'labels': tensor([1., 0., 0., 1., 0., 0.])}\n",
      "{'target_items': tensor([ 2,  2,  2,  2,  2,  2, 10, 10, 10]), 'context_items': tensor([ 0, 10,  5, 11, 11,  7,  2,  1,  9]), 'labels': tensor([1., 1., 0., 0., 0., 0., 1., 0., 0.])}\n",
      "{'target_items': tensor([9, 9, 9, 8, 8, 8, 8, 8, 8]), 'context_items': tensor([8, 7, 1, 9, 2, 0, 5, 4, 5]), 'labels': tensor([1., 0., 0., 1., 1., 0., 0., 0., 0.])}\n",
      "{'target_items': tensor([2, 2, 2]), 'context_items': tensor([8, 7, 6]), 'labels': tensor([1., 0., 0.])}\n",
      "___\n"
     ]
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, collate_fn=dataset.collate_fn\n",
    ")\n",
    "\n",
    "# Example of iterating through DataLoader\n",
    "for _ in range(2):\n",
    "    for batch in dataloader:\n",
    "        print(batch)\n",
    "    print(\"___\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61d9102-a5f8-4067-b579-312cccd1f826",
   "metadata": {},
   "source": [
    "# Try SkipGram implementation on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661dfab1-f7ec-495a-9787-a2c531b947a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
